{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjrjYNXTQnRD"
      },
      "source": [
        "## Домашнее задание №7\n",
        "\n",
        "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), @neychev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mE-o7ZrHQnRP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjaonB5nQnRU"
      },
      "source": [
        "### Задача №1:\n",
        "Обратимся к классической задаче распознавания рукописных цифр. Мы будем работать с набором данных [MNIST](http://yann.lecun.com/exdb/mnist/). В данном задании воспользуемся всем датасетом целиком.\n",
        "\n",
        "__Ваша основная задача: реализовать весь пайплан обучения модели и добиться качества $\\geq 92\\%$ на тестовой выборке.__\n",
        "\n",
        "Код для обучения модели в данном задании отсутствует. Присутствует лишь несколько тестов, которые помогут вам отладить свое решение. За примером можно обратиться к ноутбуку первого занятия.\n",
        "\n",
        "Настоятельно рекомендуем написать код \"с нуля\", лишь поглядывая на готовые примеры, а не просто \"скопировать-вставить\". Это поможет вам в дальнейшем."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "UdGtk9-jQnRW",
        "outputId": "c51d65d7-9076-4bc2-ee89-4736c99e151c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Image label: 1')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "\n",
        "train_mnist_data = MNIST('.', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
        "test_mnist_data = MNIST('.', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
        "\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_mnist_data,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    test_mnist_data,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "random_batch = next(iter(train_data_loader))\n",
        "_image, _label = random_batch[0][0], random_batch[1][0]\n",
        "plt.figure()\n",
        "plt.imshow(_image.reshape(28, 28))\n",
        "plt.title(f'Image label: {_label}')\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCmjuF66QnRY"
      },
      "source": [
        "Постройте модель ниже. Пожалуйста, не стройте переусложненную сеть, не стоит делать ее глубже четырех слоев (можно и меньше). Ваша основная задача – обучить модель и получить качество на отложенной (тестовой выборке) не менее 92% accuracy.\n",
        "\n",
        "*Комментарий: для этого достаточно линейных слоев и функций активации.*\n",
        "\n",
        "__Внимание, ваша модель должна быть представлена именно переменной `model`.__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "1QTLVEjsHWE7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, in_features, hidden_units, out_features):\n",
        "        super().__init__()\n",
        "        self.stack = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_features, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=hidden_units*27*27, out_features=out_features)\n",
        "        )\n",
        "    #     self.flatten = nn.Flatten()\n",
        "    #     self.linear_relu_stack = nn.Sequential(\n",
        "    #         nn.Linear(28*28, 512),\n",
        "    #         nn.ReLU(),\n",
        "    #         nn.Linear(512, 512),\n",
        "    #         nn.ReLU(),\n",
        "    #         nn.Linear(512, 10)\n",
        "    #     )\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #     x = self.flatten(x)\n",
        "    #     logits = self.linear_relu_stack(x)\n",
        "    #     return logits\n",
        "    def forward(self, X):\n",
        "        return self.stack(X)\n",
        "\n",
        "model = Model(1, 8, 10).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7xWZPpSHYYd",
        "outputId": "dcce0622-11a9-407b-9eff-9783d69d0d3c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Model(\n",
            "  (stack): Sequential(\n",
            "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Flatten(start_dim=1, end_dim=-1)\n",
            "    (6): Linear(in_features=5832, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "input_image = torch.rand(3,25,25)\n",
        "print(input_image.size())"
      ],
      "metadata": {
        "id": "wWdOVdBVIJwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "KKc1p5ZtJi0R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "L7_qOKRoJmVo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    for X, y in dataloader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        test_loss += loss_fn(pred, y).item()\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "WUPS5DTKJnHB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_data_loader, model, loss_fn, optimizer)\n",
        "    test(test_data_loader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzHUhdcDJvdN",
        "outputId": "822f0c52-86cd-447b-8f21-5132660591ef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.293745  [   32/60000]\n",
            "loss: 2.250609  [ 3232/60000]\n",
            "loss: 2.180439  [ 6432/60000]\n",
            "loss: 2.049407  [ 9632/60000]\n",
            "loss: 1.790216  [12832/60000]\n",
            "loss: 1.445991  [16032/60000]\n",
            "loss: 1.224230  [19232/60000]\n",
            "loss: 0.861920  [22432/60000]\n",
            "loss: 0.629750  [25632/60000]\n",
            "loss: 0.502261  [28832/60000]\n",
            "loss: 0.441359  [32032/60000]\n",
            "loss: 0.403289  [35232/60000]\n",
            "loss: 0.564721  [38432/60000]\n",
            "loss: 0.424243  [41632/60000]\n",
            "loss: 0.431653  [44832/60000]\n",
            "loss: 0.404983  [48032/60000]\n",
            "loss: 0.412143  [51232/60000]\n",
            "loss: 0.432565  [54432/60000]\n",
            "loss: 0.304141  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.8%, Avg loss: 0.352226 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.276413  [   32/60000]\n",
            "loss: 0.191639  [ 3232/60000]\n",
            "loss: 0.288340  [ 6432/60000]\n",
            "loss: 0.612043  [ 9632/60000]\n",
            "loss: 0.314453  [12832/60000]\n",
            "loss: 0.255576  [16032/60000]\n",
            "loss: 0.291981  [19232/60000]\n",
            "loss: 0.268851  [22432/60000]\n",
            "loss: 0.115360  [25632/60000]\n",
            "loss: 0.131745  [28832/60000]\n",
            "loss: 0.324970  [32032/60000]\n",
            "loss: 0.513453  [35232/60000]\n",
            "loss: 0.465422  [38432/60000]\n",
            "loss: 0.338812  [41632/60000]\n",
            "loss: 0.397735  [44832/60000]\n",
            "loss: 0.258062  [48032/60000]\n",
            "loss: 0.618308  [51232/60000]\n",
            "loss: 0.141904  [54432/60000]\n",
            "loss: 0.510261  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 90.9%, Avg loss: 0.310484 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.176632  [   32/60000]\n",
            "loss: 0.336898  [ 3232/60000]\n",
            "loss: 0.525177  [ 6432/60000]\n",
            "loss: 0.786353  [ 9632/60000]\n",
            "loss: 0.231195  [12832/60000]\n",
            "loss: 0.293719  [16032/60000]\n",
            "loss: 0.359719  [19232/60000]\n",
            "loss: 0.256543  [22432/60000]\n",
            "loss: 0.336060  [25632/60000]\n",
            "loss: 0.202710  [28832/60000]\n",
            "loss: 0.168129  [32032/60000]\n",
            "loss: 0.654830  [35232/60000]\n",
            "loss: 0.271127  [38432/60000]\n",
            "loss: 0.282844  [41632/60000]\n",
            "loss: 0.432819  [44832/60000]\n",
            "loss: 0.223372  [48032/60000]\n",
            "loss: 0.575332  [51232/60000]\n",
            "loss: 0.188576  [54432/60000]\n",
            "loss: 0.209097  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 91.6%, Avg loss: 0.296606 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.338307  [   32/60000]\n",
            "loss: 0.246211  [ 3232/60000]\n",
            "loss: 0.253283  [ 6432/60000]\n",
            "loss: 0.505632  [ 9632/60000]\n",
            "loss: 0.366976  [12832/60000]\n",
            "loss: 0.324079  [16032/60000]\n",
            "loss: 0.200656  [19232/60000]\n",
            "loss: 0.234757  [22432/60000]\n",
            "loss: 0.746212  [25632/60000]\n",
            "loss: 0.347798  [28832/60000]\n",
            "loss: 0.291365  [32032/60000]\n",
            "loss: 0.211934  [35232/60000]\n",
            "loss: 0.153308  [38432/60000]\n",
            "loss: 0.160301  [41632/60000]\n",
            "loss: 0.199857  [44832/60000]\n",
            "loss: 0.188196  [48032/60000]\n",
            "loss: 0.508834  [51232/60000]\n",
            "loss: 0.297192  [54432/60000]\n",
            "loss: 0.071654  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 91.3%, Avg loss: 0.300119 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.497016  [   32/60000]\n",
            "loss: 0.191855  [ 3232/60000]\n",
            "loss: 0.419775  [ 6432/60000]\n",
            "loss: 0.843656  [ 9632/60000]\n",
            "loss: 0.536838  [12832/60000]\n",
            "loss: 0.109663  [16032/60000]\n",
            "loss: 0.225652  [19232/60000]\n",
            "loss: 0.239445  [22432/60000]\n",
            "loss: 0.203017  [25632/60000]\n",
            "loss: 0.471920  [28832/60000]\n",
            "loss: 0.420161  [32032/60000]\n",
            "loss: 0.222413  [35232/60000]\n",
            "loss: 0.096719  [38432/60000]\n",
            "loss: 0.100956  [41632/60000]\n",
            "loss: 0.548342  [44832/60000]\n",
            "loss: 0.409445  [48032/60000]\n",
            "loss: 0.149772  [51232/60000]\n",
            "loss: 0.294061  [54432/60000]\n",
            "loss: 0.549923  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 91.8%, Avg loss: 0.284548 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YiDdTzzDQnRZ"
      },
      "outputs": [],
      "source": [
        "# Creating model instance\n",
        "\n",
        "model = model.to(device) # your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvcJfzy-QnRa"
      },
      "source": [
        "Локальные тесты для проверки вашей модели доступны ниже:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QEquoVJbQnRb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3459a3e6-f6eb-4375-af8f-75ac87251945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Everything seems fine!\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert model is not None, 'Please, use `model` variable to store your model'\n",
        "\n",
        "try:\n",
        "    x = random_batch[0]#.reshape(-1, 784)\n",
        "    y = random_batch[1]\n",
        "\n",
        "    # compute outputs given inputs, both are variables\n",
        "    y_predicted = model(x)\n",
        "except Exception as e:\n",
        "    print('Something is wrong with the model')\n",
        "    raise e\n",
        "\n",
        "\n",
        "assert y_predicted.shape[-1] == 10, 'Model should predict 10 logits/probas'\n",
        "\n",
        "print('Everything seems fine!')\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3V7zwrwQnRc"
      },
      "source": [
        "Настройте параметры модели на обучающей выборке. Рекомендуем поработать с различными оптимизаторами."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "z1wpzzmXQnRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfe73b07-653b-44cc-f83a-69ccf54cc464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.294163  [   32/60000]\n",
            "loss: 0.521802  [ 3232/60000]\n",
            "loss: 0.156765  [ 6432/60000]\n",
            "loss: 0.062194  [ 9632/60000]\n",
            "loss: 0.110805  [12832/60000]\n",
            "loss: 0.124759  [16032/60000]\n",
            "loss: 0.185440  [19232/60000]\n",
            "loss: 0.081034  [22432/60000]\n",
            "loss: 0.202171  [25632/60000]\n",
            "loss: 0.606714  [28832/60000]\n",
            "loss: 0.066471  [32032/60000]\n",
            "loss: 0.066596  [35232/60000]\n",
            "loss: 0.129204  [38432/60000]\n",
            "loss: 0.060008  [41632/60000]\n",
            "loss: 0.104733  [44832/60000]\n",
            "loss: 0.042695  [48032/60000]\n",
            "loss: 0.063978  [51232/60000]\n",
            "loss: 0.046363  [54432/60000]\n",
            "loss: 0.115506  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.065025 \n",
            "\n",
            "hidden_unites: 7, lr: 0.001\n",
            "Done!\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.034467  [   32/60000]\n",
            "loss: 0.331275  [ 3232/60000]\n",
            "loss: 0.281680  [ 6432/60000]\n",
            "loss: 0.191127  [ 9632/60000]\n",
            "loss: 0.094878  [12832/60000]\n",
            "loss: 0.094710  [16032/60000]\n",
            "loss: 0.040435  [19232/60000]\n",
            "loss: 0.125166  [22432/60000]\n",
            "loss: 0.101250  [25632/60000]\n",
            "loss: 0.065760  [28832/60000]\n",
            "loss: 0.400491  [32032/60000]\n",
            "loss: 0.309050  [35232/60000]\n",
            "loss: 0.027081  [38432/60000]\n",
            "loss: 0.087152  [41632/60000]\n",
            "loss: 0.021888  [44832/60000]\n",
            "loss: 0.044097  [48032/60000]\n",
            "loss: 0.018386  [51232/60000]\n",
            "loss: 0.196130  [54432/60000]\n",
            "loss: 0.020099  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.4%, Avg loss: 0.083673 \n",
            "\n",
            "hidden_unites: 7, lr: 0.01\n",
            "Done!\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.021620  [   32/60000]\n",
            "loss: 2.378172  [ 3232/60000]\n",
            "loss: 2.282058  [ 6432/60000]\n",
            "loss: 2.312659  [ 9632/60000]\n",
            "loss: 2.348742  [12832/60000]\n",
            "loss: 2.324831  [16032/60000]\n",
            "loss: 2.349581  [19232/60000]\n",
            "loss: 2.297309  [22432/60000]\n",
            "loss: 2.302414  [25632/60000]\n",
            "loss: 2.354122  [28832/60000]\n",
            "loss: 2.307258  [32032/60000]\n",
            "loss: 2.305750  [35232/60000]\n",
            "loss: 2.346700  [38432/60000]\n",
            "loss: 2.323609  [41632/60000]\n",
            "loss: 2.311698  [44832/60000]\n",
            "loss: 2.297520  [48032/60000]\n",
            "loss: 2.289341  [51232/60000]\n",
            "loss: 2.260963  [54432/60000]\n",
            "loss: 2.319888  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.3%, Avg loss: 2.315778 \n",
            "\n",
            "hidden_unites: 7, lr: 0.1\n",
            "Done!\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.320733  [   32/60000]\n",
            "loss: 0.655904  [ 3232/60000]\n",
            "loss: 0.097959  [ 6432/60000]\n",
            "loss: 0.172485  [ 9632/60000]\n",
            "loss: 0.057476  [12832/60000]\n",
            "loss: 0.070270  [16032/60000]\n",
            "loss: 0.065536  [19232/60000]\n",
            "loss: 0.138450  [22432/60000]\n",
            "loss: 0.018536  [25632/60000]\n",
            "loss: 0.081571  [28832/60000]\n",
            "loss: 0.174342  [32032/60000]\n",
            "loss: 0.077217  [35232/60000]\n",
            "loss: 0.063714  [38432/60000]\n",
            "loss: 0.013350  [41632/60000]\n",
            "loss: 0.013212  [44832/60000]\n",
            "loss: 0.077515  [48032/60000]\n",
            "loss: 0.165640  [51232/60000]\n",
            "loss: 0.159061  [54432/60000]\n",
            "loss: 0.175253  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.8%, Avg loss: 0.067427 \n",
            "\n",
            "hidden_unites: 8, lr: 0.001\n",
            "Done!\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.009010  [   32/60000]\n",
            "loss: 0.069836  [ 3232/60000]\n",
            "loss: 0.046824  [ 6432/60000]\n",
            "loss: 0.317079  [ 9632/60000]\n",
            "loss: 0.088196  [12832/60000]\n",
            "loss: 0.306806  [16032/60000]\n",
            "loss: 0.012710  [19232/60000]\n",
            "loss: 0.022783  [22432/60000]\n",
            "loss: 0.012874  [25632/60000]\n",
            "loss: 0.056693  [28832/60000]\n",
            "loss: 0.211491  [32032/60000]\n",
            "loss: 0.018051  [35232/60000]\n",
            "loss: 0.033918  [38432/60000]\n",
            "loss: 0.017283  [41632/60000]\n",
            "loss: 0.010032  [44832/60000]\n",
            "loss: 0.011215  [48032/60000]\n",
            "loss: 0.139560  [51232/60000]\n",
            "loss: 0.040151  [54432/60000]\n",
            "loss: 0.434852  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.2%, Avg loss: 0.140878 \n",
            "\n",
            "hidden_unites: 8, lr: 0.01\n",
            "Done!\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.146967  [   32/60000]\n",
            "loss: 2.257562  [ 3232/60000]\n",
            "loss: 2.323130  [ 6432/60000]\n",
            "loss: 2.324746  [ 9632/60000]\n",
            "loss: 2.297351  [12832/60000]\n",
            "loss: 2.318119  [16032/60000]\n",
            "loss: 2.342146  [19232/60000]\n",
            "loss: 2.321520  [22432/60000]\n",
            "loss: 2.283584  [25632/60000]\n",
            "loss: 2.403071  [28832/60000]\n",
            "loss: 2.288076  [32032/60000]\n",
            "loss: 2.301652  [35232/60000]\n",
            "loss: 2.278453  [38432/60000]\n",
            "loss: 2.295812  [41632/60000]\n",
            "loss: 2.366238  [44832/60000]\n",
            "loss: 2.312105  [48032/60000]\n",
            "loss: 2.346457  [51232/60000]\n",
            "loss: 2.277333  [54432/60000]\n",
            "loss: 2.272528  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.1%, Avg loss: 2.319674 \n",
            "\n",
            "hidden_unites: 8, lr: 0.1\n",
            "Done!\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.296756  [   32/60000]\n",
            "loss: 0.358777  [ 3232/60000]\n",
            "loss: 0.170590  [ 6432/60000]\n",
            "loss: 0.065477  [ 9632/60000]\n",
            "loss: 0.387406  [12832/60000]\n",
            "loss: 0.098466  [16032/60000]\n",
            "loss: 0.060306  [19232/60000]\n",
            "loss: 0.028810  [22432/60000]\n",
            "loss: 0.107651  [25632/60000]\n",
            "loss: 0.081576  [28832/60000]\n",
            "loss: 0.067413  [32032/60000]\n",
            "loss: 0.055069  [35232/60000]\n",
            "loss: 0.098287  [38432/60000]\n",
            "loss: 0.067615  [41632/60000]\n",
            "loss: 0.023821  [44832/60000]\n",
            "loss: 0.383050  [48032/60000]\n",
            "loss: 0.044920  [51232/60000]\n",
            "loss: 0.301411  [54432/60000]\n",
            "loss: 0.071528  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.058935 \n",
            "\n",
            "hidden_unites: 9, lr: 0.001\n",
            "Done!\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.005051  [   32/60000]\n",
            "loss: 0.177775  [ 3232/60000]\n",
            "loss: 0.092302  [ 6432/60000]\n",
            "loss: 0.052344  [ 9632/60000]\n",
            "loss: 0.048670  [12832/60000]\n",
            "loss: 0.169847  [16032/60000]\n",
            "loss: 15.086224  [19232/60000]\n",
            "loss: 0.318068  [22432/60000]\n",
            "loss: 0.297826  [25632/60000]\n",
            "loss: 0.024992  [28832/60000]\n",
            "loss: 0.215873  [32032/60000]\n",
            "loss: 0.135992  [35232/60000]\n",
            "loss: 0.041351  [38432/60000]\n",
            "loss: 0.022450  [41632/60000]\n",
            "loss: 0.028599  [44832/60000]\n",
            "loss: 0.152957  [48032/60000]\n",
            "loss: 0.034185  [51232/60000]\n",
            "loss: 0.034342  [54432/60000]\n",
            "loss: 0.107102  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.080504 \n",
            "\n",
            "hidden_unites: 9, lr: 0.01\n",
            "Done!\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.015032  [   32/60000]\n",
            "loss: 2.302434  [ 3232/60000]\n",
            "loss: 2.232716  [ 6432/60000]\n",
            "loss: 2.322955  [ 9632/60000]\n",
            "loss: 2.335892  [12832/60000]\n",
            "loss: 2.305981  [16032/60000]\n",
            "loss: 2.328060  [19232/60000]\n",
            "loss: 2.296063  [22432/60000]\n",
            "loss: 2.272667  [25632/60000]\n",
            "loss: 2.277537  [28832/60000]\n",
            "loss: 2.284066  [32032/60000]\n",
            "loss: 2.276160  [35232/60000]\n",
            "loss: 2.317702  [38432/60000]\n",
            "loss: 2.328524  [41632/60000]\n",
            "loss: 2.321309  [44832/60000]\n",
            "loss: 2.307816  [48032/60000]\n",
            "loss: 2.321027  [51232/60000]\n",
            "loss: 2.311325  [54432/60000]\n",
            "loss: 2.312813  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.3%, Avg loss: 2.312176 \n",
            "\n",
            "hidden_unites: 9, lr: 0.1\n",
            "Done!\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.313720  [   32/60000]\n",
            "loss: 0.859501  [ 3232/60000]\n",
            "loss: 0.475051  [ 6432/60000]\n",
            "loss: 0.349316  [ 9632/60000]\n",
            "loss: 0.358539  [12832/60000]\n",
            "loss: 0.204788  [16032/60000]\n",
            "loss: 0.124795  [19232/60000]\n",
            "loss: 0.158953  [22432/60000]\n",
            "loss: 0.194014  [25632/60000]\n",
            "loss: 0.108023  [28832/60000]\n",
            "loss: 0.320374  [32032/60000]\n",
            "loss: 0.035765  [35232/60000]\n",
            "loss: 0.065059  [38432/60000]\n",
            "loss: 0.074659  [41632/60000]\n",
            "loss: 0.125511  [44832/60000]\n",
            "loss: 0.037852  [48032/60000]\n",
            "loss: 0.036421  [51232/60000]\n",
            "loss: 0.013187  [54432/60000]\n",
            "loss: 0.017798  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.0%, Avg loss: 0.061619 \n",
            "\n",
            "hidden_unites: 10, lr: 0.001\n",
            "Done!\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.035935  [   32/60000]\n",
            "loss: 0.252256  [ 3232/60000]\n",
            "loss: 0.439227  [ 6432/60000]\n",
            "loss: 0.105296  [ 9632/60000]\n",
            "loss: 0.401058  [12832/60000]\n",
            "loss: 0.338431  [16032/60000]\n",
            "loss: 0.017398  [19232/60000]\n",
            "loss: 0.093996  [22432/60000]\n",
            "loss: 0.003026  [25632/60000]\n",
            "loss: 0.045847  [28832/60000]\n",
            "loss: 0.157493  [32032/60000]\n",
            "loss: 0.032769  [35232/60000]\n",
            "loss: 0.066808  [38432/60000]\n",
            "loss: 0.160970  [41632/60000]\n",
            "loss: 0.007701  [44832/60000]\n",
            "loss: 0.283861  [48032/60000]\n",
            "loss: 0.050906  [51232/60000]\n",
            "loss: 0.039865  [54432/60000]\n",
            "loss: 0.010863  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.3%, Avg loss: 0.219318 \n",
            "\n",
            "hidden_unites: 10, lr: 0.01\n",
            "Done!\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.339382  [   32/60000]\n",
            "loss: 1.721244  [ 3232/60000]\n",
            "loss: 1.934345  [ 6432/60000]\n",
            "loss: 1.957487  [ 9632/60000]\n",
            "loss: 1.402382  [12832/60000]\n",
            "loss: 0.623654  [16032/60000]\n",
            "loss: 0.668958  [19232/60000]\n",
            "loss: 1.013494  [22432/60000]\n",
            "loss: 1.053148  [25632/60000]\n",
            "loss: 0.550794  [28832/60000]\n",
            "loss: 1.131804  [32032/60000]\n",
            "loss: 0.209929  [35232/60000]\n",
            "loss: 2.323975  [38432/60000]\n",
            "loss: 2.306219  [41632/60000]\n",
            "loss: 2.303295  [44832/60000]\n",
            "loss: 2.347795  [48032/60000]\n",
            "loss: 2.335520  [51232/60000]\n",
            "loss: 2.318651  [54432/60000]\n",
            "loss: 2.335439  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 9.8%, Avg loss: 2.312119 \n",
            "\n",
            "hidden_unites: 10, lr: 0.1\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "# model has one hyperparameter - quantity of hidden units\n",
        "epochs = 1\n",
        "for hp_hidden_units in [7, 8, 9, 10]:\n",
        "    model_test = Model(1, hp_hidden_units, 10).to(device)\n",
        "    for lr in [1e-3, 1e-2, 1e-1]:\n",
        "        optimizer_rmsprob = torch.optim.RMSprop(model_test.parameters(), lr=lr)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        for t in range(epochs):\n",
        "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "            train(train_data_loader, model_test, loss_fn, optimizer_rmsprob)\n",
        "            test(test_data_loader, model_test, loss_fn)\n",
        "            print(f'hidden_unites: {hp_hidden_units}, lr: {lr}')\n",
        "        print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# best:\n",
        "# Accuracy: 98.1%, Avg loss: 0.058935\n",
        "# hidden_unites: 9, lr: 0.001"
      ],
      "metadata": {
        "id": "2cSstZfiGrC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "for hp_hidden_units in [7, 8, 9, 10]:\n",
        "    model_test = Model(1, hp_hidden_units, 10).to(device)\n",
        "    for lr in [1e-3, 1e-2]:\n",
        "        optimizer_adam = torch.optim.Adam(model_test.parameters(), lr=lr)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        for t in range(epochs):\n",
        "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "            train(train_data_loader, model_test, loss_fn, optimizer_adam)\n",
        "            test(test_data_loader, model_test, loss_fn)\n",
        "            print(f'hidden_unites: {hp_hidden_units}, lr: {lr}')\n",
        "        print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2TpgBbDB_OS",
        "outputId": "24b18697-2210-485a-8ee9-b90ee8b36388"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.300851  [   32/60000]\n",
            "loss: 0.370973  [ 3232/60000]\n",
            "loss: 0.775004  [ 6432/60000]\n",
            "loss: 0.214979  [ 9632/60000]\n",
            "loss: 0.404402  [12832/60000]\n",
            "loss: 0.135372  [16032/60000]\n",
            "loss: 0.141843  [19232/60000]\n",
            "loss: 0.303949  [22432/60000]\n",
            "loss: 0.095861  [25632/60000]\n",
            "loss: 0.083996  [28832/60000]\n",
            "loss: 0.237843  [32032/60000]\n",
            "loss: 0.086285  [35232/60000]\n",
            "loss: 0.114346  [38432/60000]\n",
            "loss: 0.299254  [41632/60000]\n",
            "loss: 0.183506  [44832/60000]\n",
            "loss: 0.102819  [48032/60000]\n",
            "loss: 0.018207  [51232/60000]\n",
            "loss: 0.262098  [54432/60000]\n",
            "loss: 0.018598  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.079792 \n",
            "\n",
            "hidden_unites: 7, lr: 0.001\n",
            "Done!\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.086866  [   32/60000]\n",
            "loss: 0.455763  [ 3232/60000]\n",
            "loss: 0.041001  [ 6432/60000]\n",
            "loss: 0.013904  [ 9632/60000]\n",
            "loss: 0.093256  [12832/60000]\n",
            "loss: 0.049080  [16032/60000]\n",
            "loss: 0.330556  [19232/60000]\n",
            "loss: 0.018419  [22432/60000]\n",
            "loss: 0.015305  [25632/60000]\n",
            "loss: 0.041594  [28832/60000]\n",
            "loss: 0.018299  [32032/60000]\n",
            "loss: 0.193411  [35232/60000]\n",
            "loss: 0.009642  [38432/60000]\n",
            "loss: 0.045074  [41632/60000]\n",
            "loss: 0.052951  [44832/60000]\n",
            "loss: 0.465528  [48032/60000]\n",
            "loss: 0.038761  [51232/60000]\n",
            "loss: 0.101010  [54432/60000]\n",
            "loss: 0.014896  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.081139 \n",
            "\n",
            "hidden_unites: 7, lr: 0.01\n",
            "Done!\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.297664  [   32/60000]\n",
            "loss: 0.537046  [ 3232/60000]\n",
            "loss: 0.193793  [ 6432/60000]\n",
            "loss: 0.442043  [ 9632/60000]\n",
            "loss: 0.026150  [12832/60000]\n",
            "loss: 0.091956  [16032/60000]\n",
            "loss: 0.407701  [19232/60000]\n",
            "loss: 0.757563  [22432/60000]\n",
            "loss: 0.039847  [25632/60000]\n",
            "loss: 0.230701  [28832/60000]\n",
            "loss: 0.297010  [32032/60000]\n",
            "loss: 0.065181  [35232/60000]\n",
            "loss: 0.042938  [38432/60000]\n",
            "loss: 0.067395  [41632/60000]\n",
            "loss: 0.045325  [44832/60000]\n",
            "loss: 0.034693  [48032/60000]\n",
            "loss: 0.067869  [51232/60000]\n",
            "loss: 0.058205  [54432/60000]\n",
            "loss: 0.071779  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.7%, Avg loss: 0.069331 \n",
            "\n",
            "hidden_unites: 8, lr: 0.001\n",
            "Done!\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.170739  [   32/60000]\n",
            "loss: 0.088026  [ 3232/60000]\n",
            "loss: 0.040921  [ 6432/60000]\n",
            "loss: 0.025353  [ 9632/60000]\n",
            "loss: 0.022490  [12832/60000]\n",
            "loss: 0.089417  [16032/60000]\n",
            "loss: 0.360946  [19232/60000]\n",
            "loss: 0.010138  [22432/60000]\n",
            "loss: 0.102276  [25632/60000]\n",
            "loss: 0.123540  [28832/60000]\n",
            "loss: 0.028838  [32032/60000]\n",
            "loss: 0.285123  [35232/60000]\n",
            "loss: 0.183410  [38432/60000]\n",
            "loss: 0.261529  [41632/60000]\n",
            "loss: 0.103324  [44832/60000]\n",
            "loss: 0.001854  [48032/60000]\n",
            "loss: 0.051917  [51232/60000]\n",
            "loss: 0.023652  [54432/60000]\n",
            "loss: 0.012345  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.069298 \n",
            "\n",
            "hidden_unites: 8, lr: 0.01\n",
            "Done!\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.305668  [   32/60000]\n",
            "loss: 0.301772  [ 3232/60000]\n",
            "loss: 0.162678  [ 6432/60000]\n",
            "loss: 0.166694  [ 9632/60000]\n",
            "loss: 0.173128  [12832/60000]\n",
            "loss: 0.363613  [16032/60000]\n",
            "loss: 0.026647  [19232/60000]\n",
            "loss: 0.203200  [22432/60000]\n",
            "loss: 0.044077  [25632/60000]\n",
            "loss: 0.065838  [28832/60000]\n",
            "loss: 0.040597  [32032/60000]\n",
            "loss: 0.144031  [35232/60000]\n",
            "loss: 0.064036  [38432/60000]\n",
            "loss: 0.082762  [41632/60000]\n",
            "loss: 0.141813  [44832/60000]\n",
            "loss: 0.016736  [48032/60000]\n",
            "loss: 0.202968  [51232/60000]\n",
            "loss: 0.113369  [54432/60000]\n",
            "loss: 0.059986  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.7%, Avg loss: 0.074100 \n",
            "\n",
            "hidden_unites: 9, lr: 0.001\n",
            "Done!\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.007681  [   32/60000]\n",
            "loss: 0.102928  [ 3232/60000]\n",
            "loss: 0.133744  [ 6432/60000]\n",
            "loss: 0.033160  [ 9632/60000]\n",
            "loss: 0.120841  [12832/60000]\n",
            "loss: 0.073197  [16032/60000]\n",
            "loss: 0.043209  [19232/60000]\n",
            "loss: 0.156096  [22432/60000]\n",
            "loss: 0.014292  [25632/60000]\n",
            "loss: 0.121579  [28832/60000]\n",
            "loss: 0.129628  [32032/60000]\n",
            "loss: 0.002709  [35232/60000]\n",
            "loss: 0.163843  [38432/60000]\n",
            "loss: 0.180267  [41632/60000]\n",
            "loss: 0.004187  [44832/60000]\n",
            "loss: 0.014698  [48032/60000]\n",
            "loss: 0.007344  [51232/60000]\n",
            "loss: 0.001142  [54432/60000]\n",
            "loss: 0.256969  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.1%, Avg loss: 0.092159 \n",
            "\n",
            "hidden_unites: 9, lr: 0.01\n",
            "Done!\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.336471  [   32/60000]\n",
            "loss: 0.237550  [ 3232/60000]\n",
            "loss: 0.175911  [ 6432/60000]\n",
            "loss: 0.082691  [ 9632/60000]\n",
            "loss: 0.241693  [12832/60000]\n",
            "loss: 0.241667  [16032/60000]\n",
            "loss: 0.026050  [19232/60000]\n",
            "loss: 0.067366  [22432/60000]\n",
            "loss: 0.115321  [25632/60000]\n",
            "loss: 0.299526  [28832/60000]\n",
            "loss: 0.048974  [32032/60000]\n",
            "loss: 0.033959  [35232/60000]\n",
            "loss: 0.022701  [38432/60000]\n",
            "loss: 0.027667  [41632/60000]\n",
            "loss: 0.017615  [44832/60000]\n",
            "loss: 0.025223  [48032/60000]\n",
            "loss: 0.123622  [51232/60000]\n",
            "loss: 0.075403  [54432/60000]\n",
            "loss: 0.067284  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.075178 \n",
            "\n",
            "hidden_unites: 10, lr: 0.001\n",
            "Done!\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.040728  [   32/60000]\n",
            "loss: 0.231510  [ 3232/60000]\n",
            "loss: 0.091192  [ 6432/60000]\n",
            "loss: 0.022442  [ 9632/60000]\n",
            "loss: 0.004323  [12832/60000]\n",
            "loss: 0.098534  [16032/60000]\n",
            "loss: 0.049076  [19232/60000]\n",
            "loss: 0.020603  [22432/60000]\n",
            "loss: 0.067974  [25632/60000]\n",
            "loss: 0.491416  [28832/60000]\n",
            "loss: 0.067063  [32032/60000]\n",
            "loss: 0.007151  [35232/60000]\n",
            "loss: 0.046270  [38432/60000]\n",
            "loss: 0.208592  [41632/60000]\n",
            "loss: 0.101850  [44832/60000]\n",
            "loss: 0.049195  [48032/60000]\n",
            "loss: 0.196507  [51232/60000]\n",
            "loss: 0.016392  [54432/60000]\n",
            "loss: 0.012067  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.076519 \n",
            "\n",
            "hidden_unites: 10, lr: 0.01\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# best\n",
        "# Test Error:\n",
        "# Accuracy: 97.9%, Avg loss: 0.069298\n",
        "# hidden_unites: 8, lr: 0.01"
      ],
      "metadata": {
        "id": "u3t2tVGsIS8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total best:\n",
        "# optimizer: rmsprop\n",
        "# hidden_units: 9\n",
        "# lr: 0.001"
      ],
      "metadata": {
        "id": "aHX175zBIwlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = Model(1, 9, 10).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(final_model.parameters(), 1e-3)"
      ],
      "metadata": {
        "id": "1oGRbXY6JDjX"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_data_loader, final_model, loss_fn, optimizer)\n",
        "    test(test_data_loader, final_model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Eulj5ScJnHB",
        "outputId": "2a0efa4e-c22b-48da-e53b-544e44f9adae"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.306481  [   32/60000]\n",
            "loss: 0.572943  [ 3232/60000]\n",
            "loss: 0.139508  [ 6432/60000]\n",
            "loss: 0.277421  [ 9632/60000]\n",
            "loss: 0.213714  [12832/60000]\n",
            "loss: 0.063244  [16032/60000]\n",
            "loss: 0.070716  [19232/60000]\n",
            "loss: 0.106243  [22432/60000]\n",
            "loss: 0.043550  [25632/60000]\n",
            "loss: 0.033887  [28832/60000]\n",
            "loss: 0.033500  [32032/60000]\n",
            "loss: 0.309914  [35232/60000]\n",
            "loss: 0.071020  [38432/60000]\n",
            "loss: 0.025918  [41632/60000]\n",
            "loss: 0.059871  [44832/60000]\n",
            "loss: 0.057761  [48032/60000]\n",
            "loss: 0.058803  [51232/60000]\n",
            "loss: 0.016944  [54432/60000]\n",
            "loss: 0.070918  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.067805 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.066068  [   32/60000]\n",
            "loss: 0.123702  [ 3232/60000]\n",
            "loss: 0.005357  [ 6432/60000]\n",
            "loss: 0.144577  [ 9632/60000]\n",
            "loss: 0.007278  [12832/60000]\n",
            "loss: 0.080232  [16032/60000]\n",
            "loss: 0.227653  [19232/60000]\n",
            "loss: 0.106656  [22432/60000]\n",
            "loss: 0.021598  [25632/60000]\n",
            "loss: 0.006668  [28832/60000]\n",
            "loss: 0.009165  [32032/60000]\n",
            "loss: 0.043844  [35232/60000]\n",
            "loss: 0.055236  [38432/60000]\n",
            "loss: 0.154903  [41632/60000]\n",
            "loss: 0.015078  [44832/60000]\n",
            "loss: 0.005942  [48032/60000]\n",
            "loss: 0.014180  [51232/60000]\n",
            "loss: 0.058749  [54432/60000]\n",
            "loss: 0.047468  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.049322 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.013195  [   32/60000]\n",
            "loss: 0.013133  [ 3232/60000]\n",
            "loss: 0.160734  [ 6432/60000]\n",
            "loss: 0.008884  [ 9632/60000]\n",
            "loss: 0.007046  [12832/60000]\n",
            "loss: 0.028579  [16032/60000]\n",
            "loss: 0.010372  [19232/60000]\n",
            "loss: 0.003822  [22432/60000]\n",
            "loss: 0.030496  [25632/60000]\n",
            "loss: 0.247488  [28832/60000]\n",
            "loss: 0.159615  [32032/60000]\n",
            "loss: 0.011366  [35232/60000]\n",
            "loss: 0.056526  [38432/60000]\n",
            "loss: 0.049352  [41632/60000]\n",
            "loss: 0.091475  [44832/60000]\n",
            "loss: 0.059786  [48032/60000]\n",
            "loss: 0.008961  [51232/60000]\n",
            "loss: 0.120419  [54432/60000]\n",
            "loss: 0.155209  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.4%, Avg loss: 0.048407 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.017186  [   32/60000]\n",
            "loss: 0.005802  [ 3232/60000]\n",
            "loss: 0.001447  [ 6432/60000]\n",
            "loss: 0.016660  [ 9632/60000]\n",
            "loss: 0.002016  [12832/60000]\n",
            "loss: 0.006069  [16032/60000]\n",
            "loss: 0.103014  [19232/60000]\n",
            "loss: 0.004438  [22432/60000]\n",
            "loss: 0.080345  [25632/60000]\n",
            "loss: 0.022428  [28832/60000]\n",
            "loss: 0.000989  [32032/60000]\n",
            "loss: 0.001302  [35232/60000]\n",
            "loss: 0.026883  [38432/60000]\n",
            "loss: 0.021936  [41632/60000]\n",
            "loss: 0.122203  [44832/60000]\n",
            "loss: 0.036089  [48032/60000]\n",
            "loss: 0.004858  [51232/60000]\n",
            "loss: 0.130239  [54432/60000]\n",
            "loss: 0.114675  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.045923 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.049174  [   32/60000]\n",
            "loss: 0.037737  [ 3232/60000]\n",
            "loss: 0.019024  [ 6432/60000]\n",
            "loss: 0.108058  [ 9632/60000]\n",
            "loss: 0.001352  [12832/60000]\n",
            "loss: 0.103518  [16032/60000]\n",
            "loss: 0.002260  [19232/60000]\n",
            "loss: 0.001048  [22432/60000]\n",
            "loss: 0.001944  [25632/60000]\n",
            "loss: 0.011155  [28832/60000]\n",
            "loss: 0.003357  [32032/60000]\n",
            "loss: 0.012566  [35232/60000]\n",
            "loss: 0.063190  [38432/60000]\n",
            "loss: 0.001483  [41632/60000]\n",
            "loss: 0.004128  [44832/60000]\n",
            "loss: 0.003110  [48032/60000]\n",
            "loss: 0.047656  [51232/60000]\n",
            "loss: 0.000495  [54432/60000]\n",
            "loss: 0.034644  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.045060 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = final_model"
      ],
      "metadata": {
        "id": "5sqOtM25LXkG"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppSdeU-YQnRf"
      },
      "source": [
        "Также, напоминаем, что в любой момент можно обратиться к замечательной [документации](https://pytorch.org/docs/stable/index.html) и [обучающим примерам](https://pytorch.org/tutorials/).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grdHchH_QnRf"
      },
      "source": [
        "Оценим качество классификации:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "o1NTzu-vQnRg"
      },
      "outputs": [],
      "source": [
        "predicted_labels = []\n",
        "real_labels = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in train_data_loader:\n",
        "        y_predicted = model(batch[0])#.reshape(-1, 784))\n",
        "        predicted_labels.append(y_predicted.argmax(dim=1))\n",
        "        real_labels.append(batch[1])\n",
        "\n",
        "predicted_labels = torch.cat(predicted_labels)\n",
        "real_labels = torch.cat(real_labels)\n",
        "train_acc = (predicted_labels == real_labels).type(torch.FloatTensor).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "9Za0okGwQnRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d27d6607-81fa-4e28-9c51-ddd83c61e264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural network accuracy on train set: 0.99213\n"
          ]
        }
      ],
      "source": [
        "print(f'Neural network accuracy on train set: {train_acc:3.5}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "T5zwf7u0QnRh"
      },
      "outputs": [],
      "source": [
        "predicted_labels = []\n",
        "real_labels = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_data_loader:\n",
        "        y_predicted = model(batch[0])#.reshape(-1, 784))\n",
        "        predicted_labels.append(y_predicted.argmax(dim=1))\n",
        "        real_labels.append(batch[1])\n",
        "\n",
        "predicted_labels = torch.cat(predicted_labels)\n",
        "real_labels = torch.cat(real_labels)\n",
        "test_acc = (predicted_labels == real_labels).type(torch.FloatTensor).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "w-n7xHw2QnRi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98e2903e-7b44-4d10-e7a2-cb1c5b048d64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural network accuracy on test set: 0.9873\n"
          ]
        }
      ],
      "source": [
        "print(f'Neural network accuracy on test set: {test_acc:3.5}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmVIhaH2QnRj"
      },
      "source": [
        "Проверка, что необходимые пороги пройдены:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "85P-Ye83QnRj"
      },
      "outputs": [],
      "source": [
        "assert test_acc >= 0.92, 'Test accuracy is below 0.92 threshold'\n",
        "assert train_acc >= 0.91, 'Train accuracy is below 0.91 while test accuracy is fine. We recommend to check your model and data flow'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rxvCpSNQnRk"
      },
      "source": [
        "### Сдача задания\n",
        "Загрузите файл `hw07_data_dict.npy` (ссылка есть на странице с заданием) и запустите код ниже для генерации посылки. Код ниже может его загрузить (но в случае возникновения ошибки скачайте и загрузите его вручную)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Z3Aknb6LQnRk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "530e3be4-b025-43f7-92c0-829aa5dba39f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-19 15:10:08--  https://raw.githubusercontent.com/girafe-ai/ml-course/23s_dd_ml/homeworks/hw07_mnist_classification/hw07_data_dict.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6272438 (6.0M) [application/octet-stream]\n",
            "Saving to: ‘hw07_data_dict.npy’\n",
            "\n",
            "hw07_data_dict.npy  100%[===================>]   5.98M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-08-19 15:10:08 (67.1 MB/s) - ‘hw07_data_dict.npy’ saved [6272438/6272438]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/23s_dd_ml/homeworks/hw07_mnist_classification/hw07_data_dict.npy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "import os\n",
        "\n",
        "assert os.path.exists('hw07_data_dict.npy'), 'Please, download `hw07_data_dict.npy` and place it in the working directory'\n",
        "\n",
        "def get_predictions(model, eval_data, step=10):\n",
        "\n",
        "    predicted_labels = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx in range(0, len(eval_data), step):\n",
        "            y_predicted = model(eval_data[idx:idx+step].reshape(-1, 784))\n",
        "            predicted_labels.append(y_predicted.argmax(dim=1))\n",
        "\n",
        "    predicted_labels = torch.cat(predicted_labels)\n",
        "    return predicted_labels\n",
        "\n",
        "loaded_data_dict = np.load('hw07_data_dict.npy', allow_pickle=True)\n",
        "\n",
        "submission_dict = {\n",
        "    'train': get_predictions(model, torch.FloatTensor(loaded_data_dict.item()['train'])).numpy(),\n",
        "    'test': get_predictions(model, torch.FloatTensor(loaded_data_dict.item()['test'])).numpy()\n",
        "}\n",
        "\n",
        "np.save('submission_dict_hw07.npy', submission_dict, allow_pickle=True)\n",
        "print('File saved to `submission_dict_hw07.npy`')\n",
        "# __________end of block__________"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drEwCCyyPAlm",
        "outputId": "b3d6efe9-6f46-41d4-8fc4-d27154f41aa9"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved to `submission_dict_hw07.npy`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuIU7ON1QnRl"
      },
      "source": [
        "На этом задание завершено. Поздравляем!"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0vlMMxXwN5dq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Py3 Research",
      "language": "python",
      "name": "py3_research"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}